{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/swc/Multimedia-Final/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import CLIPProcessor, CLIPTextModel, CLIPTokenizer, CLIPModel\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPModel.from_pretrained(\n",
    "    \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\", torch_dtype=torch.float16\n",
    ").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\n",
    "    \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\", torch_dtype=torch.float16\n",
    ")\n",
    "model_vision = model.vision_model\n",
    "visual_projection = model.visual_projection\n",
    "model_text = model.text_model\n",
    "text_projection = model.text_projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1024, out_features=1024, bias=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_vision.requires_grad_(False)\n",
    "model_text.requires_grad_(False)\n",
    "text_projection.requires_grad_(False)\n",
    "model_text.eval()\n",
    "model_vision.eval()\n",
    "text_projection.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    def __init__(self, path):\n",
    "        print(f\"Prepare data for {path}\")\n",
    "        self.animal_type = os.listdir(path)\n",
    "        self.animal_type.sort()\n",
    "        animal_type_prefix = [f\"a photo of {animal}\" for animal in self.animal_type]\n",
    "        self.images = []\n",
    "        for i in tqdm(range(len(self.animal_type))):\n",
    "            animal = self.animal_type[i]\n",
    "            image_list = os.listdir(f\"{path}/{animal}\")\n",
    "            image_list.sort()\n",
    "            images = [Image.open(f\"{path}/{animal}/{file}\") for file in image_list]\n",
    "            inputs = processor(\n",
    "                text=animal_type_prefix,\n",
    "                images=images,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "            )\n",
    "            self.images.append(inputs.pixel_values)\n",
    "        inputs = inputs.to(device)\n",
    "        self.titles = text_projection(\n",
    "            model_text(input_ids=inputs.input_ids).pooler_output\n",
    "        )\n",
    "        self.images = torch.cat(self.images)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Preprocess image using CLIP's preprocessing function\n",
    "        image = self.images[idx]\n",
    "        title = self.titles[idx // (len(self.images) // len(self.animal_type))]\n",
    "        return (\n",
    "            image,\n",
    "            title,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare data for topic2_release/train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:18<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare data for topic2_release/test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.06it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset(path=\"topic2_release/train\")\n",
    "test_dataset = dataset(path=\"topic2_release/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=1e-3, betas=(0.9, 0.98), eps=1e-4\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.5)\n",
    "MSELoss = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 100\n",
    "start = 0\n",
    "loss_training = []\n",
    "loss_testing = []\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir_name = 'output_model'\n",
    "if not os.path.isdir(model_dir_name):\n",
    "    os.mkdir(model_dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_resume = False\n",
    "\n",
    "if is_resume:\n",
    "    resume_epoch = 0\n",
    "    start = resume_epoch + 1\n",
    "    model = CLIPModel.from_pretrained(\n",
    "        f\"{model_dir_name}/epoch_{resume_epoch}\", torch_dtype=torch.float16\n",
    "    ).to(device)\n",
    "    optimizer.load_state_dict(\n",
    "        torch.load(f\"{model_dir_name}/epoch_{resume_epoch}/optimizer.bin\")\n",
    "    )\n",
    "    scheduler.load_state_dict(\n",
    "        torch.load(f\"{model_dir_name}/epoch_{resume_epoch}/scheduler.bin\")\n",
    "    )\n",
    "    with open(f\"{model_dir_name}/epoch_{resume_epoch}/loss_history.pkl\", \"rb\") as handle:\n",
    "        save_loss = pickle.load(handle)\n",
    "        loss_training = save[\"training\"]\n",
    "        loss_testing = save[\"testing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batches = [batch for batch in test_dataloader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(start, num_epochs):\n",
    "    pbar = tqdm(train_dataloader, total=len(train_dataloader))\n",
    "    for i, batch in enumerate(pbar):\n",
    "        # model_vision.train()\n",
    "        visual_projection.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images, texts = batch\n",
    "\n",
    "        images = images.to(device)\n",
    "        texts = texts.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        # output = model(pixel_values=images, input_ids=texts)\n",
    "        x = model_vision(pixel_values=images)\n",
    "        x = visual_projection(x.pooler_output)\n",
    "        y = texts\n",
    "        # y = model_text(input_ids=texts)\n",
    "        # y = text_projection(y.pooler_output)\n",
    "        # Compute loss\n",
    "        loss = MSELoss(x, y)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_training.append(loss.item())\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            # model_vision.eval()\n",
    "            visual_projection.eval()\n",
    "            test_batch = test_batches[i // 10]\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            mages, texts = test_batch\n",
    "            images = images.to(device)\n",
    "            texts = texts.to(device)\n",
    "\n",
    "            # Forward passs\n",
    "            x = model_vision(pixel_values=images)\n",
    "            x = visual_projection(x.pooler_output)\n",
    "            y = texts\n",
    "            # y = model_text(input_ids=texts)\n",
    "            # y = text_projection(y.pooler_output)\n",
    "            \n",
    "            loss = MSELoss(x, y)\n",
    "            loss_testing.append(loss.item())\n",
    "\n",
    "        pbar.set_description(f\"Epoch: {epoch}/{num_epochs}, Training loss: {loss_training[-1]:.5f}, Testing loss: {loss_testing[-1]:.5f}\")\n",
    "    scheduler.step()\n",
    "    \n",
    "    model.save_pretrained(f\"{model_dir_name}/epoch_{epoch}/\")\n",
    "    torch.save(\n",
    "        optimizer.state_dict(),\n",
    "        f\"{model_dir_name}/epoch_{epoch}/optimizer.bin\",\n",
    "    )\n",
    "    torch.save(\n",
    "        scheduler.state_dict(),\n",
    "        f\"{model_dir_name}/epoch_{epoch}/scheduler.bin\",\n",
    "    )  \n",
    "\n",
    "    with open(f\"output_model/epoch_{epoch}/loss_history.pkl\", \"wb\") as handle:\n",
    "        save_loss = {\"training\": loss_training, \"testing\": loss_testing}\n",
    "        pickle.dump(save_loss, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
